{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3004,"databundleVersionId":861823,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\n- In this notebook, i will be giving a comprehensive introduction and implementation to how CNN is implemented using only basic libraries such as numpy and cupy.\n\n- CNN performs very well in image related tasks, such as image recognition, object detection.\n\n- This notebook can be served as teaching and learning purpose, most of the code here will be the basis of my future research in computer vision and convolutional neural network. Lets take a look on the details of convolutional neural network first !","metadata":{}},{"cell_type":"markdown","source":"# Key terms\n1. Feature : **Features are some characteristics in training data which they help doing the task** .We believe machine learns features from training data to finish the task very accurately, such as textures, colours, edges from images. Extracting features is an important goal of machine learning, by extracting feataures machine can do the assigned task very accurately. In CNN, feature maps usually mean the matrix after convolution and max-pooling. Which they contains features, some we may understand how they help doing the task, but mostly, we dont.\n\n2. Learning Rate(Or step size) : When we are doing gradient descent optimization, we will calculate the gradient of loss against weight, then update the weight, step size measures **how big the step will be**, which is how the name \"step size\" come.\n\n3. Bias : In machine learning, bias means the model has **bias or prejudice** to the input data, which might leads to inaccuarate prediction.","metadata":{}},{"cell_type":"markdown","source":"# Inspiration of CNN\n- We can dive deeper into how human eyes work before we introduce CNN. In accordance to Hubel and Wiesel's (1959) research, they found that the visual cortex has small regions of cells that are sensitive to specific regions in the visual field. Which suggested that our eye might **not** be attentive to everywhere in our visual field.\n\n- Hence, this inspires how can we make a model to process images, we first make a **kernel**, which its dimension is small compared to the whole image, then we slides the kernel in the image to get some features in the image to simulate how our eyes catch the light details of an object. Then we can send these \"signals\"(feature maps in machine learning) to our brain for further processings, such as identification and classification.\n  ","metadata":{}},{"cell_type":"markdown","source":"**1. Convolution**\n- Convolution serves a major part in convolutional neural network, it is the \"soul\" of CNN, lets start from its mathematical definition, the conovlution of 2 functions f(t) and g(t) is defined as :\n\n  $$f(t) \\ast g(t) = \\int^{\\infty}_{-\\infty} f(\\tau)g(t-\\tau) d\\tau$$\n\nThe above definition can be generalized to grid-structured data such as matrix, in fact, in CNN, we use matrix to represent data more frequently, hence, we are able to define convolution for matrix, which is **dot product** of 2 matrices, because the convolution formula becomes :\n\n$$Y(i,j) = \\sum^{h-1}_{m=0} \\sum^{w-1}_{n=1} X(i+m, j+n)\\cdot W(m,n)$$\n\nWhich is a element-wise dot product.\n\n\n**2. Image representation**\n- How do we represent images in computer? There are 3 major components in image representation, which are **channels, height, width**. Height and Width are very easy to understand, what about channels?\n\n- Well, we know that the most basic part in an image is **pixel**, **channel can be imagined as how many layers of rectangles you stacked where all these layers of rectangles have the same height and width**.\n\n- For example, if channel = 1, it is simply grayscale image. If channel=3, it is RGB images, where you have 3 layers of rectangles here, Red, Green and Blue one. Each layer has same dimension. More channels allow the feature map to store more features but also lead to tremendous increment on memory usage and computational requirement, hence increasing channel is a trade-off between computational efficiency and model representation power.","metadata":{}},{"cell_type":"markdown","source":"# > **CNN architecture**\n\n# 1. **Convolution**\n\nWhen we have received an input image, we will immediately do convolution on the images, so what is the details?\n\n  (The following shapes representations followed by Numpy traditions on representing matrix shapes)\n\n  Suppose the input image has a shape of **(channels, height, width)**, at first, we have to initialize our filters into shape of **(output_channels, input_channels, kernel_height, kernel_width)**, for those who love math, here's for you :\n\n$$X \\in \\mathcal{R}^{H \\times W\\times C}$$\n$$W \\in \\mathcal{R}^{W_h \\times W_w \\times C_{in} \\times C_{out} \\times N}$$\n\n$C_{in}$ and $C_{out}$ means the **output and input channels of the feature map**\n\nHow about the values inside these matrices? We mostly use some popular initialization techniques, for CNN, obviously He Intialization is the best, it is proposed by He. K. Zhang et.al(2015), where the values are given by :\n\n  $$w \\sim \\mathcal{U}(0, \\sqrt{\\frac{2}{n_l}})$$\n\nThe shape of the feature map after convolution is :\n\n$$F_{q+1} = \\frac{F_q-B_q+2P}{S} + 1$$\n\nNote that it can only be applied to calculate width and height of feature map, we will explain how this formula is derived in later section.\n\nAnd the mathematical formula for convolution is :\n\n$$h^{q+1}_{rsp} = \\sum^{B_q}_{i=1} \\sum^{B_q}_{j=1} \\sum^{d_{q+1}}_{k=1} w^{(p,q)}_{rsk}\\cdot h^{q}_{rS+i-P, sS+j-P,k} = \\sum_{(i,j,k) \\in \\mathcal{K}} w^{(p,q)}_{rsk}\\cdot h^{q}_{rS+i-P, sS+j-P,k}$$\n\nFor most general case, batch is included :\n$$ = \\sum_{(i,j,k,n) \\in \\mathcal{K}} w^{(p,q)}_{rsk}\\cdot h^{q}_{rS+i-P, sS+j-P,k,n}$$\n\nIt looks very complex, but let me explain them by part ! GIF 1 gives you a more simpler explanation by demonstrating it visually !\n\n$F_q$ is the shape of feature map in qth layer, $B_q$ is the shape of kernel connecting between q and (q+1)th layer, just think of it as weights connecting 2 layers, as you use this kernel to convolve with feature map(simply doing dot product) in qth layer to get the feature map in (q+1)th layer. Here we got 2 more new variables, S and P, who are they? They are **strides and padding** !\n\n* Strides : Stride means when we are sliding filter in the feature map, **how many grids do we slide when in 1 move**, usually if stride is larger, we use less memory and computational power as we perform less dot product between kernel and selected region.\n  \nThe convolution is performed at position of :\n\n  $$1, S+1, 2S+1, .....$$ for S>1 \n  \n* In the demonstration below, it used stride=2, which means the kernel will move 2 grid in each move.\n\n\n* Padding : In the above examples of demonstration of Convolution, you may have discovered that, after the convolution, the feature map dimension will be smaller, that is the nature of convolution, but unfortunately, it is **undesirable**, because the reduce of size leads to **some loss of information**, so the goal of padding is to solve this issue, how do we make the shape between and before convolution the same? That's right, that's the add some numbers around the feature map border, usually the number is 0, by adding padding into feature map, we can retain the shape of feature map !\n\n\nNow look back at the shape formula we wrote in the beginning :\n\n$$F_{q+1} = \\frac{F_q-B_q+2P}{S} + 1$$\n\n$F_q - B_q$ is caused by dot product, **only one scalar value will be returned after the dot product**, we simply stack these values together to create a feature map !\n\nSuppose we add padding with thickness $P$ in the border, then our height/width dimension is increased by $2P$(dimension increased P in each side), hence we have $+2P$.\n\n$/S$ and the $+1$ due to stride, because convolution start in the first position of the feature map, as we move $S$ grids at each move, the required times of convolution is divided by $S$.\n\n* **Little Conclusion of this part**:\n  - Stride : How many grids your kernel moves in 1 convolution.\n  - Padding : Thickness of zeros you add in the border of your feature map, pad=0 means no padding, in most libraries, \"same\" padding meaning that the spatial dimensions of feature map before and after convolution is the same, which suggested that it uses zero-padding to retain the shape, for \"valid\" padding, it simply equals to no use of padding. ","metadata":{}},{"cell_type":"markdown","source":"* **im2col method for convolution**\n  \nNext, we are going to introduce an important algorithm to calculate convolution in CNN, which is called im2col(image to columns) technique, it is proposed by *Chellapilla, K., Puri, S., & Simard, P. (2006)*, the basic idea is to turn the kernel and feature maps into **column matrix**, hence we can change the series of dot product into matrix multiplication, then we can use the optimized BLAS library to boost the convolution. \n\n1. Flatten the kernel\n   \nBy the following command, we can flatten our kernel :\n\n**np.reshape(kernel, (out_channel ,-1))**\n\n2. Slides over the feature map\n\nNext we have to let our original kernel to slides over the feature map like normal convolution, but instead of performing **dot product**, we are going to reshape the **selected region** into column matrix.\n\nDefine a region proposal as $R$, where it is matrix with shape :\n\n\nAfter we finished this process, we can stack all the column matrix together, the number of rows indicate **how many times our kernel has slides over the feature map**.\n\n3. Matrix Multiplication\n\nIn real situation, with consideration to the channels, our column matrix should be a 2D array, as well as kernel. Hence, in the final step, we simply multiply the flattened kernel with the column matrix to get the feature map !\n\nMathematically:\n$$columns = \\left\\{ X_{\\text{padded}}[:, :, i \\cdot s : i \\cdot s + HH, j \\cdot s : j \\cdot s + WW] \\mid \\forall N, \\forall C, 0 \\leq i < \\text{out}_h, 0 \\leq j < \\text{out}_w \\right\\}.\\text{reshape}(C\\times HH\\times WW, N*out_h*out_w)$$\nOr you can understand as stacking columns together into a new matrix for all possible (i,j) position:\n$$columns = [col_1, col_2,....,col_{C\\times HH\\times WW}]$$\noutput = $$W.\\text{reshape}(OC, IC\\times HH\\times WW) \\times columns$$\nWhich is much faster than:\n$$ = \\sum_{(i,j,k,n) \\in \\mathcal{K}} w^{(p,q)}_{rsk}\\cdot h^{q}_{rS+i-P, sS+j-P,k,n}$$\n\ndue to the computing power of GPU and mitigation of explicit for loops \n\n\nHere is the implementation of im2col : \n","metadata":{}},{"cell_type":"code","source":"#Implementation of im2col and conv2d\n#Number of rows = times of convolution\nimport cupy as cp \n\ndef im2col_cupy(X_train, kernel, stride, pad):\n    # Ensure the kernel and input have matching in_channels\n    # Flatten the kernel\n    k_flatten = cp.reshape(kernel, newshape=(kernel.shape[0], -1))\n\n    # Calculate output shape with given padding and strides\n    N, C, H, W = X_train.shape\n    OC, IC, HH, WW = kernel.shape\n    assert IC == C \n    #Input channel of kernel and channel of feature map input channel must be the same \n\n    out_h, out_w = (H-HH+2*pad)//stride + 1, (W-WW+2*pad)//stride + 1\n\n    # Pad the input if necessary\n    if pad > 0:\n        X_padded = cp.pad(X_train, pad_width=[(0, 0), (0, 0), (pad, pad), (pad, pad)], mode='constant')\n    else:\n        X_padded = X_train\n\n    # Generate column matrix using efficient CuPy operations\n    columns = cp.array([\n        X_padded[:, :, i*stride:i*stride+HH,j*stride:j*stride+WW]\n        for i in range(0, out_h)\n        for j in range(0, out_w)\n    ])\n    col = columns.reshape((C*HH*WW, N*out_h*out_w))\n    columns_reshaped = columns.reshape((N, C, HH, WW, out_h, out_w))\n    del columns \n\n    #Use safe_matmul, safe_matmul can divide the a, b into chunks to do matrix multiplication in case memory is insufficient\n    try:\n        a = cp.matmul(k_flatten, col).reshape((N,OC,out_h,out_w))\n    except cp.cuda.memory.OutOfMemoryError:\n        r = []\n        chunks = 2\n        arrays = cp.array([col[:, i*(col.shape[1])//chunks:(i+1)*(col.shape[1]//chunks)] for i in range(chunks)])\n        #Transform into array with shape of (chunks, ....)\n        for i in range(chunks):\n            with cp.cuda.Device(0):\n                m = cp.matmul(k_flatten, arrays[i, :, :])\n            r.append(m)\n            #Concatenate the small chunks together to make the original matrix \n        del arrays \n        a = cp.concatenate(a, axis=1).reshape((N, OC, out_h, out_w))\n        \n    return columns_reshaped, k_flatten, a\n    #Note that i have integrated the function of convolution into this program so it can directly output convolved feature map, so we dont need another conv2d function\nX = cp.random.randn(1000, 3, 30, 30)\nW = cp.random.randn(100, 3, 3, 3)\nim2col_cupy(X, W, stride=1, pad=1)    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Later we also need **col2im** in CNN backpropagation, here is an implementation of col2im, we are simply reversing the process !\n\nThus, studying the code yourself can boost your understanding on numpy libraries and matrices operations !","metadata":{}},{"cell_type":"code","source":"def col2im(cols:cp.array, input, kernel, stride, pad):\n\n    N, C, H, W = input.shape\n    _, _, HH, WW = kernel.shape \n    out_h = (H+2*pad-HH)//stride + 1\n    out_w = (W+2*pad-WW)//stride + 1\n    cols = cols.reshape(N*C, HH*WW, out_h, out_w)\n    X = cp.zeros((N,C,H+2*pad,W+2*pad))\n    #Place an assertion to detect incompitable shapes early \n\n    for i in range(HH): \n        for j in range(WW): \n            X[:, :, i:i+stride*out_h:stride, j:j+stride*out_w:stride] += cols[:, i*WW+j, :, :].reshape(N, C, out_h, out_w)\n\n    if pad>0:\n        X = X[:, :, pad:-pad, pad:-pad]\n    return X ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. **Max-Pooling**\n-  Max-Pooling is another important concept in CNN, for large feature maps, the memory and computational power it takes to run the convolution is tremendous. Aside from insufficient memory, as we put more layers in, running convolution takes up more time, that is undesirable.\n\n- Max-Pooling is a method to reduce **feature maps dimension**, the basic idea is to make up a \"pool\", then select the maximum values in the pool to represent the whole pool, it can reduce the dimension of feature map very effectively.\n\nMathematically, it is equivalent to :\n- $$ h^{\\text{out}}_{n, c, p, q} = \\max_{\\substack{0 \\le i < PH \\\\ 0 \\le j < PW}} h^{\\text{in}}_{n, c, S \\cdot p + i, S \\cdot q + j} $$\n\n- Same as convolution, we have 2 important parameters in Max-Pooling layer, they are **pool_size and strides**\n\n- **Stride** : Same as the stride we introduced before, describing how many grids the pool will move in the feature map to perform max pooling.\n\n- **Pool_size** : Pool size means the dimension of the pool, it must be a square, such as (3,3) or (4,4), hence for simplicity we often just mention the first number.\n  \nHere is the implementation of Max-Pooling :","metadata":{}},{"cell_type":"code","source":"def maxpooling(input, size, stride):\n        batch_size, in_channel, in_height, _ = input.shape[:]\n        output_size = (in_height - size)//stride\n\n        o = cp.array([\n                input[:, :, i*stride:i*stride+size, j*stride:j*stride+size]\n                      for i in range(output_size) for j in range(output_size)]\n        ).reshape((batch_size, in_channel, output_size, output_size, size, size)).max(axis=(4,5))\n#Use list comprehension to execute the maxpooling algorithm \n        assert o.shape == (batch_size, in_channel, output_size, output_size)\n\n        return o","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Backpropagation in CNN\nThe backpropagation in CNN is just a more complicated version of chain rule in MLP, lets start with the backpropagation in MLP !\n\n**1. MLP backpropagation**\n \n   We can have numerous layers in MLP, but the fundamental concept behind these formulas are the same, which is the chain rule, we give the formulas **for 2 hidden layers**, you can generalize them to nth layers.\n   The update formula of the weight is :\n\n   Hence, by using chain rule of differentiation, we have:\n$$\\delta_3 = \\frac{\\partial L}{\\partial Z_3} = \\frac{\\partial L}{\\partial h_3} \\cdot \\frac{\\partial h_3}{\\partial Z_3}$$\n$$\\delta_2 = \\frac{\\partial L}{\\partial Z_2} = \\frac{\\partial L}{\\partial h_3} \\cdot \\frac{\\partial h_3}{\\partial Z_3} \\cdot \\frac{\\partial Z_3}{\\partial h_2} \\cdot \\frac{\\partial h_2}{\\partial Z_2} = \\delta_3 \\cdot \\frac{\\partial Z_3}{\\partial h_2} \\cdot \\frac{\\partial h_2}{\\partial Z_2}$$\n$$\\delta_1 = \\frac{\\partial L}{\\partial Z_1} = \\frac{\\partial L}{\\partial h_3} \\cdot \\frac{\\partial h_3}{\\partial Z_3} \\cdot \\frac{\\partial Z_3}{\\partial h_2} \\cdot \\frac{\\partial h_2}{\\partial Z_2} \\cdot \\frac{\\partial Z_2}{\\partial h_1} \\cdot \\frac{\\partial h_1}{\\partial Z_1} = \\delta_2 \\cdot \\frac{\\partial Z_2}{\\partial h_1} \\cdot \\frac{\\partial h_1}{\\partial Z_1}$$\n\nBy calculating the partial derivatives, we can get:\n$$\\delta_3 = \\sigma'(Z_3)\\cdot (\\hat{y}-y)$$\n$$\\delta_2 = \\delta_3 \\cdot W_3^T\\cdot \\sigma'(Z_2)$$\n$$\\delta_1 = \\delta_2 \\cdot W_2^T \\cdot \\sigma'(Z_1)$$\n\nHence, by considering the following fact deduced from chain rule:\n$$\\frac{\\partial L}{\\partial W_3} = \\frac{\\partial L}{\\partial Z_3}\\cdot \\frac{\\partial Z_3}{\\partial W_3} = \\delta_3\\cdot h_2^T$$\n$$\\frac{\\partial L}{\\partial W_2} = \\frac{\\partial L}{\\partial Z_2}\\cdot \\frac{\\partial Z_2}{\\partial W_2} = \\delta_2 \\cdot h_1^T$$\n$$\\frac{\\partial L}{\\partial W_1} = \\frac{\\partial L}{\\partial Z_1}\\cdot \\frac{\\partial Z_1}{\\partial W_1} = \\delta_1 \\cdot X^{T}$$\n\nWe can deduce the weight update formula for MLP:\n$$W_3 \\Leftarrow W_3 - \\alpha \\frac{\\partial L}{\\partial W_3}$$\n$$W_2 \\Leftarrow W_2 - \\alpha \\frac{\\partial L}{\\partial W_2}$$\n$$W_1 \\Leftarrow W_1 - \\alpha \\frac{\\partial L}{\\partial W_1}$$\r\n","metadata":{}},{"cell_type":"code","source":"h=0\n#Avoid auto running because the following code is just for demo, variables are not defined \nif h>0:\n    error = self.output - self.one_hot\n    self.delta_3 = error.T  # Shape (10, 10500)\n    self.dw_3 = cp.dot(self.delta_3, self.h_2.T)  # Shape (10, 512)\n    self.db_3 = cp.sum(self.delta_3, axis=1, keepdims=True)  # Shape (10, 1)\n    self.delta_2 = cp.dot(self.w_3.T, self.delta_3) * self.d_relu_mlp(self.sum_2)  # Shape (512, 10500)\n    self.dw_2 = cp.dot(self.delta_2, self.h_1.T)  # Shape (512, 256)\n    self.db_2 = cp.sum(self.delta_2, axis=1, keepdims=True)  # Shape (512, 1)\n    self.delta_1 = cp.dot(self.w_2.T, self.delta_2) * self.d_relu_mlp(self.sum_1)  # Shape (256, 10500)\n    self.dw_1 = cp.dot(self.delta_1, self.output_c)  # Shape (256, 3200)\n    self.db_1 = cp.sum(self.delta_1, axis=1, keepdims=True)  # Shape (256, 1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**2. Max-Pooling backpropagation**\n\n   For max-pooling layer, the mathematical representation of its BP is trivial, we only output gradient on the grid which is the max-pooling position, to do that, we can denote a 2-dimensional matrix called \"Mask\", mathematically we use the Kronecker delta as a **determinastic function**, where its value is given by:\n$$\\delta_{ij}=\n\\begin{dcases}\n    1,\\text{if (i,j) on max-pooling position}\\\\\n    0,\\text{otherwise}\n\\end{dcases}$$\n\nHence, we can multiply this Kronecker delta with the gradient to calculate the gradient with respect to the feature map in previous layer:\n\n$$\\frac{\\partial L}{\\partial x_{ij}} = \\frac{\\partial L}{\\partial \\hat{y_{ij}}} \\delta_{ij}$$","metadata":{}},{"cell_type":"code","source":"def maxpooling_backward(self, input, output, output_error_grad, size, stride):\n        batch_size, in_channel, in_height, in_width = input.shape[:]\n        output_height, output_width = output.shape[2], output.shape[3]\n        input_error_grad = cp.zeros_like(input)\n        for i in range(output_height):\n            for j in range(output_width):\n                region = input[:, :, i * stride:(i * stride) + size, j * stride:(j * stride) + size]\n                max_region = cp.max(region, axis=(2, 3), keepdims=True)\n                # Calculating max-pooling is easy as we demonstrated, so we simply add one line here to calculate maximum rather than calling function again.\n                # We need the feature map before and after max pooling for comparison of position of maximum values\n                # There is no gradient wrt to non-maximum values, the neuron who achieved maximum in max-pooling stage will gain gradient, all other neurons will get a value of zero.\n                mask = (region == max_region)\n                # Declare mask as a generator\n                input_error_grad[:, :, i * stride:(i * stride) + size,\n                j * stride:(j * stride) + size] += mask * output_error_grad[:, :, i, j][:, :, None, None]\n        return input_error_grad","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"3. Convolution backpropagation\n\n- The backpropagation in convolution layer is way more complex than other layers.\n\n  (Note : The following derivation is not very rigorous)\n\nWe can use $\\mathcal{H}$ and $\\mathcal{K}$ to represent the collection of all points in the (q-1)th layer and qth layer:\n\n\n$$\\mathcal{H} = \\{h^{q-1}_{ijk} : 0\\leq i \\leq F_{q-1} ; 0\\leq j\\leq F_{q-1} ;0\\leq k \\leq d_{q-1}\\}$$\n$$\\mathcal{K} = \\{h^{q}_{rsp}: 0\\leq r \\leq \\frac{F_{q-1} - B_{q} +2P}{S}+1 ,s\\leq \\frac{F_{q-1} - B_{q} +2P}{S}+1; 0\\leq p\\leq d_{q}\\}$$\n\nDenote the weight tensor undergoing convolution between layer (q-1) and layer q is denoted as :\n\n$$W^{q} = [w^{(d_{q},q)}_{B_{q},B_{q},d_{q-1}}]$$\nWhich means we have a weight tensor that has a shape of $B_{q-1} \\times B_{q-1} \\times d_{q-1}\\times d_q$, which the next feature map will gain a depth of $d_q$ as we have total number of $d_q$ filters.\n\nThe basic idea is to take the derivative of (q)th layer against the kernel w:\n\n$$\\frac{\\partial L}{\\partial w^{q}_{mno}} = \\sum_{(r,s,p)\\in \\mathcal{K}} \\frac{\\partial L}{\\partial o^{q}_{rsp}} \\frac{\\partial o^{q}_{rsp}}{\\partial h^{q}_{rsp}} \\frac{\\partial h^q_{rsp}}{\\partial w^q_{mno}}$$\n\nFor simplicity, we use the $delta$ to represent the gradient instead:\n$$\\delta^{q}_{rsp}=\\frac{\\partial L}{\\partial o^{q}_{rsp}}$$\n\nThe expression can further be simplified to :\n$$\\frac{\\partial L}{\\partial w^{q}_{mno}} = \\sum_{(r,s,p)\\in \\mathcal{K}} \\delta^{q}_{rsp} \\frac{\\partial o^q_{rsp}}{\\partial h^q_{rsp}}\\frac{\\partial h^{q}_{rsp}}{\\partial w^q_{mno}}$$\n\nReferring to the previous feature map calculation, we would like to expand the formula, according to the previous formula, we can find out the $\\frac{\\partial h^{q}_{rsp}}{\\partial w^q_{mno}}$:\n\n$$\\frac{\\partial h^{q}_{rsp}}{\\partial w^q_{mno}} = o^{q-1}_{rS+m-P, sS+n-P,o}$$\n\nThe partial derivative is trivial:\n\n$$\\frac{\\partial L}{\\partial w^q_{mno}} = \\sum_{(r,s,p)\\in \\mathcal{K}} \\sigma'(h^q_{rsp}) (\\delta^{q}_{rsp} \\cdot o^{q-1}_{rS+m-P, sS+n-P,o})$$\n\nThe upcoming calculation is not complex, the partial derivative against the weight is:\n\n$$\\frac{\\partial L}{\\partial w^q_{mno}} = \\sum_{(r,s,p)\\in \\mathcal{K}} \\sigma'(h^q_{rsp})\\delta^{q}_{rsp} \\cdot h^{q-1} = \\sigma'(h^q_{rsp}) \\text{rot}_{180} \\{\\delta^{q}_{rsp}\\} \\ast h^{q-1}$$\n\nFor the loss against the input layer, we are finding the gradient of loss against the (q-1)th layer, by using the chain rule, we can find that:\n\n$$\\frac{\\partial L}{\\partial h^{(q-1)}_{ijk}} = \\sum_{(r,s,p)\\in \\mathcal{K}} \\frac{\\partial L}{\\partial h^{q}_\\mathcal{K}} \\frac{\\partial h^{q}_\\mathcal{K}}{\\partial h^{q-1}_{ijk}}$$\n\nIn this phase we are simply differentiating the successive hidden layer with respect to the previous layer, take a close look at the latter partial derivative:\n$$\\frac{\\partial h^{q}_\\mathcal{K}}{\\partial h^{q-1}_{ijk}} = \\frac{\\partial }{\\partial h^{q-1}_{ijk}} \\sum_{(i,j,k)\\in \\mathcal{H}}W^q\\cdot \\sigma(h^{q-1}_{ijk})$$\n$$= W^q \\sigma'(h^{q-1}_{ijk})$$\n\nWe can put it back into our previous equation:\n$$\\frac{\\partial L}{\\partial h^{q+1}_{ijk}} = \\sum_{(r,s,p)\\in \\mathcal{K}} \\delta^{q}_{\\mathcal{K}} W^q \\sigma'(h^{q-1})$$\nAgain, we can flip the filter and undergo the convolution operation:\n$$\\frac{\\partial L}{\\partial h^{q+1}_{ijk}} = \\text{rot}_{180}\\{\\sum_{(r,s,p)\\in \\mathcal{K}} \\delta^{q}_{\\mathcal{K}} W^q\\} \\sigma'(h^{q-1})$$\nIn an element-wise aspect, it can be written as :\n$$\\frac{\\partial L}{\\partial h^{q+1}_{ijk}} = \\delta^{q}_{ijk}\\ast \\text{rot}_{180}\\{ W^{q}\\} \\sigma'(h^{q-1}_{ijk})$$\n\nHere is the implementation to it !","metadata":{}},{"cell_type":"code","source":"def conv2d_backward(dout, X, kernel, stride, pad):\n    # Dout : Upstream derivative, (N, F, out_h, out_w)\n    # X : Input feature map shape, (N, C, H, W)\n    # W : Kernel, (OC, IC, HH, WW)\n    # stride : Number of pixels between adjacent receptive fields \n    # Pad : The number of pixels that will be used for zero padding\n\n    N, C, H, W = X.shape \n    F, _, HH, WW = kernel.shape\n\n    #Compute the output shape \n    out_h, out_w = (H-HH+2*pad)//stride +1, (W-WW+2*pad)//stride + 1 \n    cols, k_flatten, _ = im2col_cupy(X, kernel, stride, pad)\n\n    db = cp.sum(dout, axis=(0,2,3))\n\n    dout_reshaped = dout.transpose(1,2,3,0).reshape(F, -1)\n    cols_reshaped = cols.transpose(1,2,3,0,4,5).reshape(-1, N*out_h*out_w)\n\n    dW = cp.matmul(dout_reshaped, cols_reshaped.T).reshape(kernel.shape)\n    #Compute the gradient wrt to the input layer\n    dcols = cp.matmul(k_flatten.T, dout_reshaped).reshape(N, C, HH, WW, out_h, out_w)\n\n    dX = col2im(dcols, X, kernel, stride, pad)\n    assert dX.shape == X.shape\n    assert dW.shape == kernel.shape \n\n    return dX, dW, db","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"4. Program initialization\n\n- This MNIST image recongition program refers to the structue of AlexNet (Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012)), my __init__ function is :","metadata":{}},{"cell_type":"code","source":"def __init__(self, input_dim=4096, output_dim=10):\n\n# Assuming n_1 to n_5 are the appropriate in_features for the respective layers\n        self.kernel_1 = self.he_initialization(in_features=(3 * 11 * 11), shape=(96, 3, 11, 11))\n        self.kernel_2 = self.he_initialization(in_features=(96 * 5 *5), shape=(256, 96, 5, 5))\n        self.kernel_3 = self.he_initialization(in_features=(256 * 3 * 3), shape=(384, 256, 3, 3))\n        self.kernel_4 = self.he_initialization(in_features=(384 * 3 *3), shape=(384, 384, 3, 3))\n        self.kernel_5 = self.he_initialization(in_features=(384 * 3 * 3), shape=(256, 384, 3, 3))\n\n        #He Intialization to prevent gradient explosion\n        \n        self.num_neurons_1 = 4096\n        self.num_neurons_2 = 4096\n        # AlexNet fully connected layers have 4096 neurons, hence the number of parameters in fully connected layers is 16,801,792\n\n        self.w_1 = self.he_initialization(in_features=(input_dim), shape=(input_dim, self.num_neurons_1))\n        self.w_2 = self.he_initialization(in_features=(self.num_neurons_1), shape=(self.num_neurons_1, self.num_neurons_2))\n        self.w_3 = self.he_initialization(in_features=(self.num_neurons_2), shape=(output_dim, self.num_neurons_2))\n        #Debugging remark : self.w_1 - self.w_3 initialized weights are around 9e-03 - 9e-05, acceptable\n\n        self.b_1 = cp.zeros((self.num_neurons_1, 1))\n        self.b_2 = cp.zeros((self.num_neurons_2, 1))\n        self.b_3 = cp.zeros((output_dim, 1))\n\n        # Defining the filters, neurons and biases\n        if not hasattr(self, 'v_1'):\n            self.v_1 = cp.zeros_like(self.w_1)\n            self.v_2 = cp.zeros_like(self.w_2)\n            self.v_3 = cp.zeros_like(self.w_3)\n\n        self.d_1 = cp.zeros_like(self.w_1)\n        self.d_2 = cp.zeros_like(self.w_2)\n        self.d_3 = cp.zeros_like(self.w_3)\n\n        self.a_1 = cp.zeros_like(self.kernel_1)\n        self.a_2 = cp.zeros_like(self.kernel_2)\n        self.a_3 = cp.zeros_like(self.kernel_3)\n        self.a_4 = cp.zeros_like(self.kernel_4)\n        self.a_5 = cp.zeros_like(self.kernel_5)\n\n        self.c_1 = cp.zeros_like(self.kernel_1)\n        self.c_2 = cp.zeros_like(self.kernel_2)\n        self.c_3 = cp.zeros_like(self.kernel_3)\n        self.c_4 = cp.zeros_like(self.kernel_4)\n        self.c_5 = cp.zeros_like(self.kernel_5)\n\n        self.learning_rate = 0.001\n        self.epsilon = 1e-8\n        self.beta_1 = 0.9\n        self.beta_2 = 0.99\n        self.epochs = 60\n        self.batch_size = 32\n        self.l_accuracy = []\n        self.l_loss = []\n        self.loss_values = []\n        self.accuracy_values = []\n\n    # Intialization part\n    # Definitng the activation functions for the convolution and MLP\n\ndef he_initialization(self, in_features, shape): \n    limit = cp.sqrt(2 / in_features) \n    return cp.random.uniform(low=-limit, high=limit, size=shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"5. Besides, we also need to declare activation functions, which are : ","metadata":{}},{"cell_type":"code","source":"def one_hot_encoder(self, label):\n    self.one_hot = cp.zeros((label.size, 10))\n    self.one_hot[cp.arange(label.size), label] = 1\n    #One-Hot encoding\n    return self.one_hot\n\n    # One-Hot encoder to turn label to shape of (6,6)\n    @staticmethod\n    def relu(input):\n        if input is None:\n            raise TypeError(\"Input of relu activation is None\")\n        return cp.maximum(0, input)\n        # The formal definition of relu \n\n    @staticmethod\n    def d_relu(output, grad_output):\n        if output is None or grad_output is None:\n            raise TypeError(\"Output or grad_output of d_relu is None\")\n        relu_grad = (output > 0).astype(cp.float32)\n        # Why use a generator here? Because it will output 1(True) and 0(False), exactly same as derivative of relu !\n        return grad_output*relu_grad\n        '''Why we need the grad_output to compare is because, when we are using d_relu, its already in backprop phase, hence,\n        we need to know the output of relu first, then we can know which value (1 or 0) to output.\n        '''\n\n    @staticmethod\n    def d_relu_mlp(input):\n        return cp.where(input > 0, 1, 0)\n\n    @staticmethod\n    def softmax(x):\n        exp_x = cp.exp(x - cp.max(x, axis=0, keepdims=True))\n        return exp_x / cp.sum(exp_x, axis=0, keepdims=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We also give the definitions of the activation functions we use here :\n\n- ReLU(Rectifier) :\n  $$ReLU(x) = \\begin{dcases}\n    x,&,\\text{if}  x\\geq 0\\\\\n    0,&,\\text{if}  x<0\n\\end{dcases}$$\n\n- Softmax :\n  $$\\sigma(x_n) = \\frac{e^{x_n}}{\\sum^{n}_{i=1} e^{x_i}}$$","metadata":{}},{"cell_type":"markdown","source":"6. **Adam Optimizer**\n- Adam optimization is proposed by Kingma, D. P., & Ba, J. (2014), which is one of the most effective optimization algorithm in the world, the reason is it combined momentum learning method(Polyak, B. T., 1964) and RMSProp(Tieleman, T., & Hinton, G. (2012))\n\n* Momentum method :\n\n We understand that the goal of deep learning is simply **finding a weight tensor such that the loss function of this weight is lowest**, hence, instead of directly updating the weight, we can instead store, aggregate and weight them :\n\n  $$V \\Leftarrow -\\alpha \\frac{\\partial L}{\\partial W}$$\n  $$V \\Leftarrow \\beta V - \\alpha \\frac{\\partial L}{\\partial W}$$\n  $$W \\Leftarrow W+V$$\n\n* RMSProp Method :\n\n  The RMSProp weight the squared gradient using a decay factor, and square root to normalize the aggregated gradeints :\n\n  $$A_i \\Leftarrow \\rho A_i + (1-\\rho)(\\frac{\\partial L}{\\partial w_i})^2 \\forall i $$\n  $$w_i \\Leftarrow w_i - \\frac{\\alpha}{\\sqrt{A_i}} (\\frac{\\partial L}{\\partial w_i})^2$$\n\n  One of the advantage of RMSProp is that the gradient will decay over time\n\n* Adam Method:\n\n  After we understood RMSProp and Momentum, we can move to Adam, Adam(Adaptive moment) combines both momentum and RMSProp, in Adam, we have first order and second order moment, which is $F_i$ and $A_i$, for the second order moment $A_i$ :\n  $$A_i \\Leftarrow \\rho A_i + (1-\\rho)(\\frac{\\partial L}{\\partial w_i})^2$$\n\n  For first order moment, we have another decay rate $\\rho_f$:\n  $$F_i \\Leftarrow \\rho_f F_i + (1-\\rho_f)(\\frac{\\partial L}{\\partial w_i})$$\n\nThe weight update formula becomes : \n$$w_i \\Leftarrow w_i - \\alpha_t \\frac{F_i}{\\sqrt{A_i}}$$\n\nWhere:\n\n$$\\alpha _t = \\alpha \\frac{\\sqrt{1-\\rho^t} }{1-\\rho_{f}^t}$$\n\nThe reason to update the learning rate is to **correct bias**, biases often appear when our training data is not diverse enough or the model is overfitting. In Adam, we initialized $A_i$ and $F_i$ to 0, so it must have caused some biases initially.\n\n","metadata":{}},{"cell_type":"code","source":"def update_params(self, t):\n    # Update for CNN kernels using Adam, where a_1 means the first order moment, c_1 means second order moment, t = iterations\n    self.a_1 = self.beta_1 * self.a_1 + (1 - self.beta_1) * self.dw_conv_1\n    self.c_1 = self.beta_2 * self.c_1 + (1 - self.beta_2) * cp.square(self.dw_conv_1)\n    m_hat_1 = self.a_1 / (1 - cp.power(self.beta_1, t) + self.epsilon)\n    v_hat_1 = self.c_1 / (1 - cp.power(self.beta_2, t) + self.epsilon)\n\n    self.kernel_1 -= self.learning_rate * m_hat_1 / (cp.sqrt(v_hat_1) + epsilon)\n    #Updating kernel in Convolution layer \n\n    self.v_1 = self.beta_1 * self.v_1 + (1 - self.beta_1) * self.dw_1\n    self.d_1 = self.beta_2 * self.d_1 + (1 - self.beta_2) * cp.square(self.dw_1)\n    m_hat_w1 = self.v_1 / (1 - cp.power(self.beta_1, t) + self.epsilon)\n    v_hat_w1 = self.d_1 / (1 - cp.power(self.beta_2, t) + self.epsilon)\n    #Updating weights in MLP layer \n    \n    self.w_1 -= self.learning_rate * m_hat_w1 / (cp.sqrt(v_hat_w1) + epsilon)\n    #The update logic in other layers is exactly the same, so we only demonstrate the update of one layer for your reference","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"7. **Batch Normalization(Optional)**\n\nHere we refer to C.C.Aggarwal[1, Sec 4.8], we can break up the addition and activation into pre-activation and post-activation, the batch normalization simply add a normalization layer in between them to normalize the input of batch elements, which can prevent many common problems such as exploding and vanishing gradient problems, first, we need to calculate the mean and standard deviation, assume we have m instances in a batch:\n\n$$\\mu_i = \\frac{\\sum^{m}_{r=1} v^{(r)}_i}{m} \\forall i$$\n$$\\sigma^2_{i} = \\frac{\\sum^{m}_{r=1} (v^(r)_i - \\mu_i)^2}{m} + \\varepsilon \\forall i$$\n\nThe new batch element value is given by:\n$$\\hat{v}^{r}_i = \\frac{v^(r)_i - \\mu_i}{\\sigma_i} \\forall i,r$$\n\nThe pre-activation value is given by:\n$$a^{(r)}_i = \\gamma_i \\cdot \\hat{v^(r)_i} + \\beta_i \\forall i,r$$\n\nWe also give the formula for the backpropagation here in order to ensure mathematical rigour, the keypoint of batch-normalization is that we also need to update the parameter $\\beta_i$ and $\\gamma_i$ for optimal performance, this can be done by using gradient descent, by chain rule, it is apparent that:\n\n$$\\frac{\\partial L}{\\partial \\beta_i} = \\sum^{m}_{r=1} \\frac{\\partial L}{\\partial a^{(r)}_i} \\cdot \\frac{\\partial a^{(r)}_i}{\\partial \\beta_i} \n= \\sum^{m}_{r=1} \\frac{\\partial L}{\\partial a^{(r)}_i}$$\n\n$$\\frac{\\partial L}{\\partial \\gamma_i} = \\sum^{m}_{r=1} \\frac{\\partial L}{\\partial a^{(r)}_i} \\cdot \\frac{\\partial a^{(r)}_i}{\\partial \\gamma_i} \n= \\sum^{m}_{r=1} \\frac{\\partial L}{\\partial a^{(r)}_i} \\cdot \\hat{v^(r)_i}$$\n\\newpage \nOur objective is to calculate $\\frac{\\partial L}{\\partial v^{(r)}}_i$ for further back-propagation, therefore, we need to calculate the 3 partial derivatives, the derivations are trivial, hence we give the formula directly:\n\n$$\\frac{\\partial L}{\\partial v_i^{(r)}} = \n\\frac{\\partial L}{\\partial a_i^{(r)}} \\left( \\frac{\\gamma_i}{\\sigma_i} \\right) + \\frac{\\partial L}{\\partial \\mu_i} \\left( \\frac{1}{m} \\right) + \\frac{\\partial L}{\\partial \\sigma_i^2} \\left( \\frac{2(v_i^{(r)} - \\mu_i)}{m} \\right)$$\n\n$$\\frac{\\partial L}{\\partial \\sigma_i^2} = \\sum_{q=1}^{m} \\frac{\\partial L}{\\partial \\hat{v}_i^{(q)}} \\cdot \\frac{\\partial \\hat{v}_i^{(q)}}{\\partial \\sigma_i^2} $$\n\n$$= -\\frac{1}{2\\sigma_i^3} \\sum_{q=1}^{m} \\frac{\\partial L}{\\partial \\hat{v}_i^{(q)}} (v_i^{(q)} - \\mu_i) $$\n$$= -\\frac{1}{2\\sigma_i^3} \\sum_{q=1}^{m} \\frac{\\partial L}{\\partial a_i^{(q)}} \\gamma_i \\cdot (v_i^{(q)} - \\mu_i)\n$$\n\n\n$$\\frac{\\partial L}{\\partial \\mu_i} = \\sum_{q=1}^{m} \\frac{\\partial L}{\\partial \\hat{v}_i^{(q)}} \\cdot \\frac{\\partial \\hat{v}_i^{(q)}}{\\partial \\mu_i} + \\frac{\\partial L}{\\partial \\sigma_i^2} \\cdot \\frac{\\partial \\sigma_i^2}{\\partial \\mu_i} = $$\n\n$$-\\frac{1}{\\sigma_i} \\sum_{q=1}^{m} \\frac{\\partial L}{\\partial \\hat{v}_i^{(q)}} - 2 \\frac{\\partial L}{\\partial \\sigma_i^2} \\cdot \\frac{\\sum_{q=1}^{m} (v_i^{(q)} - \\mu_i)}{m} $$\n\n$$= -\\frac{\\gamma_i}{\\sigma_i} \\sum_{q=1}^{m} \\frac{\\partial L}{\\partial a_i^{(q)}} + $$\n\n$$\\left( \\frac{1}{\\sigma_i^3} \\cdot \\left( \\sum_{q=1}^{m} \\frac{\\partial L}{\\partial a_i^{(q)}} \\gamma_i \\cdot (v_i^{(q)} - \\mu_i) \\right) \\right) \\cdot \\left( \\frac{\\sum_{q=1}^{m} (v_i^{(q)} - \\mu_i)}{m} \\right)\n$$\n\nThe following code is a simple implementation to batch normalization and its backpropagation:","metadata":{}},{"cell_type":"code","source":"class BatchNormalization:\n    def __init__(self, num_features, epsilon=1e-5, momentum=0.9):\n        self.epsilon = epsilon\n        self.momentum = momentum\n        self.gamma = cp.ones(num_features)\n        self.beta = cp.zeros(num_features)\n        self.running_mean = cp.zeros(num_features)\n        self.running_var = cp.ones(num_features)\n\n    def forward(self, x, training=True):\n        if training:\n            batch_mean = cp.mean(x, axis=0)\n            batch_var = cp.var(x, axis=0)\n            self.x_centered = x - batch_mean\n            #We will always use the (v-mean) in backward phase, so define it first\n            self.stddev_inv = 1.0 / cp.sqrt(batch_var + self.epsilon)\n            #Add epsilon to prevent division by 0 \n            #stddev_inv = inverse of standard deviation \n            x_norm = self.x_centered * self.stddev_inv\n            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * batch_mean\n            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * batch_var\n        else:\n            x_norm = (x - self.running_mean) / cp.sqrt(self.running_var + self.epsilon)\n\n        self.out = self.gamma * x_norm + self.beta\n        return self.out\n\n    def backward(self, dout):\n        N, D = dout.shape\n\n        x_norm = self.x_centered * self.stddev_inv\n        dbeta = cp.sum(dout, axis=0)\n        dgamma = cp.sum(dout * x_norm, axis=0)\n\n        dx_norm = dout * self.gamma\n        dvar = cp.sum(dx_norm * self.x_centered * -0.5 * self.stddev_inv**3, axis=0)\n        dmean = cp.sum(dx_norm * -self.stddev_inv, axis=0) + dvar * cp.mean(-2.0 * self.x_centered, axis=0)\n\n        dx = (dx_norm * self.stddev_inv) + (dvar * 2.0 * self.x_centered / N) + (dmean / N)\n\n        return dx, dgamma, dbeta","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"You can integrate it into the CNN on your own !","metadata":{}},{"cell_type":"code","source":"import cupy as cp \nimport cupyx.distributed.array as cda \nimport pandas as pd\nimport time \nimport numpy as np \nimport sys \nimport gc\ngc.enable()\n\npath = \"/kaggle/input/digit-recognizer/train.csv\"\npath1 = \"/kaggle/input/digit-recognizer/test.csv\"\ndata = pd.read_csv(path, engine='c')\ntest_data = pd.read_csv(path1, engine='c')\ntest_data = test_data/255.0\ntest_data = cp.array(test_data).reshape(28000, 1, 28, 28)\nlabel_1 = data['label']\nlabel_1 = label_1.T\n\ndata = cp.array(data)\nm,n = data.shape\ntrain_data = data[0:m].T\nX_train = train_data[1:n]\nX_train = X_train/255.0\nX_train = X_train.reshape(42000, 1, 28, 28)\n\n\ndef one_hot_encoding(label):\n    one_hot = cp.zeros((label.size, 10))\n    one_hot[cp.arange(label.size), label]=1\n    return one_hot \n\nclass MLP:\n    def __init__(self, input_size, hidden_size, output_size):\n        # Initialize weights and biases\n        self.W1 = cp.random.randn(input_size, hidden_size) * cp.sqrt(2. / input_size)\n        self.b1 = cp.zeros(hidden_size)\n        self.W2 = cp.random.randn(hidden_size, hidden_size) * cp.sqrt(2. / hidden_size)\n        self.b2 = cp.zeros(hidden_size)\n        self.W3 = cp.random.randn(hidden_size, output_size) * cp.sqrt(2. / hidden_size)\n        self.b3 = cp.zeros(output_size)\n\n    def relu(self, x):\n        return cp.maximum(0, x)\n\n    def relu_backward(self, dout, cache):\n        dx = dout.copy()\n        dx[cache <= 0] = 0\n        return dx \n    \n    def softmax(self,x):\n        e = cp.exp(x-cp.max(x, axis=1, keepdims=True))\n        return e/cp.sum(e, axis=1, keepdims=True)\n\n    def cross_entropy_loss(self, y_pred, y_true):\n        m = y_true.shape[0]\n        L = -cp.sum(y_true*cp.log(y_pred+1e-15), axis=1)\n        loss = cp.mean(L)\n        return loss \n    \n    def forward(self, X):\n        # Forward pass\n        self.z1 = X.dot(self.W1) + self.b1\n        self.a1 = self.relu(self.z1)\n        self.z2 = self.a1.dot(self.W2) + self.b2\n        self.a2 = self.relu(self.z2)\n        self.z3 = self.a2.dot(self.W3) + self.b3\n        self.a3 = self.softmax(self.z3)  # Assuming no activation on the output layer (e.g., for regression)\n\n        return self.a3\n\n    def backward(self, X, y, shape, learning_rate=1e-3):\n        # Backward pass\n        m = y.shape[0]\n\n        # Assuming mean squared error loss\n        dz3 = (self.a3-y)/m \n\n        dW3 = self.a2.T.dot(dz3)\n        db3 = cp.sum(dz3, axis=0)\n\n        da2 = dz3.dot(self.W3.T)\n        dz2 = self.relu_backward(da2, self.z2)\n        dW2 = self.a1.T.dot(dz2)\n        db2 = cp.sum(dz2, axis=0)\n\n        da1 = dz2.dot(self.W2.T)\n        dz1 = self.relu_backward(da1, self.z1)\n        dW1 = X.T.dot(dz1)\n        db1 = cp.sum(dz1, axis=0)\n\n        self.deflatten = cp.matmul(da1, self.W1.T).reshape(shape)\n\n        # Update weights and biases\n        self.W1 -= learning_rate * dW1\n        self.b1 -= learning_rate * db1\n        self.W2 -= learning_rate * dW2\n        self.b2 -= learning_rate * db2\n        self.W3 -= learning_rate * dW3\n        self.b3 -= learning_rate * db3\n\ndef distributed_matmul(A: cp.array, B: cp.array):\n    \"\"\"\n    Performs distributed matrix multiplication of A (MxK) and B (KxN) using two GPUs.\n    Splits A into two parts along rows and multiplies each part with B on separate GPUs.\n    Combines the results to form the final (MxN) matrix.\n\n    More assertions are made to avoid illegal inputs.\n    \"\"\"\n    M, K = A.shape\n    _, N = B.shape\n\n    # Ensure inputs are valid\n    assert A is not None and B is not None, \"Input matrices cannot be None\"\n    assert K == B.shape[0], \"Matrix dimensions do not match for multiplication\"\n    assert cp.cuda.Device(0) is not None and cp.cuda.Device(1) is not None, \"No GPU on your computer or GPU is not detected\"\n\n    # Split matrix A into two parts along rows\n    split_M = M // 2\n    A1 = A[:split_M, :]  # First half of A (split_M x K)\n    A2 = A[split_M:, :]  # Second half of A (M - split_M x K)\n\n    # Create distributed arrays for A1 and A2\n    a1 = cda.distributed_array(A1, index_map={0: (slice(0, split_M), slice(0, K))})\n    a2 = cda.distributed_array(A2, index_map={1: (slice(0, M - split_M), slice(0, K))})\n\n    # Create distributed array for B (shared across GPUs)\n    b = cda.distributed_array(B, index_map={0: (slice(0, K), slice(0, N)), 1:(slice(0, K), slice(0, N))})\n\n    # Perform matrix multiplication on GPU 0\n    C1,C2 = cda.matmul(a1, b), cda.matmul(a2,b)  # C1 = A1 @ B (split_M x N)\n    C1_local = cp.array(C1.get(order='C')) # Collect result to local memory\n    C2_local = cp.array(C2.get(order='C'))\n\n    del a1, a2, b, C1, C2\n\n    # Concatenate C1 and C2 along rows to form the final result\n    C = cp.vstack((C1_local, C2_local))  # C = [C1; C2] (M x N)\n    \n    # Clean up distributed arrays\n    del C1_local, C2_local\n\n    # Verify the final shape\n    assert C.shape == (M, N), f\"Matrix multiplication error: Expected shape {(M, N)}, got {C.shape}\"\n    return C\n\ndef im2col_cupy(X_train:cp.array, kernel:cp.array, stride:int, pad:int) -> tuple:\n    # Flatten the kernel\n    k_flatten = cp.reshape(kernel, newshape=(kernel.shape[0], -1))\n\n    # Calculate output shape\n    N, C, H, W = X_train.shape\n    OC, IC, HH, WW = kernel.shape\n    out_h, out_w = (H - HH + 2 * pad) // stride + 1, (W - WW + 2 * pad) // stride + 1\n\n    # Pad the input\n    if pad > 0:\n        X_padded = cp.pad(X_train, pad_width=[(0, 0), (0, 0), (pad, pad), (pad, pad)], mode='constant')\n    else:\n        X_padded = X_train\n\n    # Generate column matrix using as_strided\n    columns = cp.lib.stride_tricks.as_strided(\n        X_padded,\n        shape=(N, C, out_h, out_w, HH, WW),\n        strides=(\n            X_padded.strides[0],\n            X_padded.strides[1],\n            X_padded.strides[2] * stride,\n            X_padded.strides[3] * stride,\n            X_padded.strides[2],\n            X_padded.strides[3]\n        )\n    ).reshape(C*HH*WW, N*out_h*out_w)\n\n    # Perform matrix multiplication\n    a = distributed_matmul(k_flatten, columns).reshape(N,OC,out_h,out_w)\n\n    return columns.reshape(N, C, HH, WW, out_h, out_w), k_flatten, a\n\ndef col2im(cols:cp.array, input:cp.array, kernel:cp.array, stride:int, pad:int) -> cp.array:\n\n    N, C, H, W = input.shape\n    _, _, HH, WW = kernel.shape \n    out_h = (H+2*pad-HH)//stride + 1\n    out_w = (W+2*pad-WW)//stride + 1\n    cols = cols.reshape(N*C, HH*WW, out_h, out_w)\n    X = cp.zeros((N,C,H+2*pad,W+2*pad))\n    cols : cp.arary \n    X : cp.array \n\n    #Place an assertion to detect incompitable shapes early \n\n    for i in range(HH): \n        for j in range(WW): \n            X[:, :, i:i+stride*out_h:stride, j:j+stride*out_w:stride] += cols[:, i*WW+j, :, :].reshape(N, C, out_h, out_w)\n\n    if pad>0:\n        X = X[:, :, pad:-pad, pad:-pad]\n    return X \n\ndef conv2d_backward(dout, X, kernel, stride, pad):\n\n    # Dout : Upstream derivative, (N, F, out_h, out_w)\n    # X : Input feature map shape, (N, C, H, W)\n    # W : Kernel, (OC, IC, HH, WW)\n    # stride : Number of pixels between adjacent receptive fields \n    # Pad : The number of pixels that will be used for zero padding\n\n    N, C, H, W = X.shape \n    F, _, HH, WW = kernel.shape\n\n    #Compute the output shape \n    out_h, out_w = (H-HH+2*pad)//stride +1, (W-WW+2*pad)//stride + 1 \n    cols, k_flatten, _ = im2col_cupy(X, kernel, stride, pad)\n\n    db = cp.sum(dout, axis=(0,2,3))\n\n    dout_reshaped = dout.transpose(1,2,3,0).reshape(F, -1)\n    cols_reshaped = cols.transpose(1,2,3,0,4,5).reshape(-1, N*out_h*out_w)\n    \n    dW = distributed_matmul(dout_reshaped, cols_reshaped.T).reshape(kernel.shape)\n    #Compute the gradient wrt to the input layer\n    dcols = distributed_matmul(k_flatten.T, dout_reshaped).reshape(N, C, HH, WW, out_h, out_w)\n\n    dX = col2im(dcols, X, kernel, stride, pad)\n    assert dX.shape == X.shape\n    assert dW.shape == kernel.shape \n\n    return dX, dW, db\n\nclass CNN:\n    def __init__(self, input_shape, num_classes):\n        # Initialize layers\n        self.conv_kernel = cp.random.randn(8, input_shape[1], 3, 3) * cp.sqrt(2. / (3 * 3 * input_shape[1]))\n        self.conv_stride = 1\n        self.conv_pad = 1\n        \n        self.pool_size = 2\n        self.pool_stride = 2\n        \n        # Calculate the size of the flattened layer after pooling\n        conv_output_height = (input_shape[2] - 3 + 2 * self.conv_pad) // self.conv_stride + 1\n        conv_output_width = (input_shape[3] - 3 + 2 * self.conv_pad) // self.conv_stride + 1\n        pool_output_height = (conv_output_height - self.pool_size) // self.pool_stride + 1\n        pool_output_width = (conv_output_width - self.pool_size) // self.pool_stride + 1\n        \n        flattened_size = 8 * pool_output_height * pool_output_width\n        \n        # Initialize MLP\n        self.mlp = MLP(flattened_size, 100, num_classes)\n\n    def relu(self, x):\n        return cp.maximum(0, x)\n\n    def relu_backward(self, dout, cache):\n        dx = dout.copy()\n        dx[cache <= 0] = 0\n        return dx \n    \n    def forward(self, X):\n        # Convolutional layer\n        self.conv_out, self.conv_cols, self.a = im2col_cupy(X, self.conv_kernel, self.conv_stride, self.conv_pad)\n        self.a_1 = self.relu(self.a)\n        \n        # Max pooling layer\n        self.pool_out = self.maxpool_forward(self.a)\n        self.flat_shape = self.pool_out.shape \n        # Flatten\n        self.flattened = self.pool_out.reshape(X.shape[0], -1)\n        \n        # Fully connected layer\n        output = self.mlp.forward(self.flattened)\n        \n        return output\n    \n    def backward(self, X, y, learning_rate=1e-3):\n        # Backward pass through MLP\n        self.mlp.backward(self.flattened, y, shape=self.flat_shape, learning_rate=learning_rate)\n        \n        # Backward pass through max pooling\n        pool_error_grad = self.mlp.deflatten\n        conv_error_grad = self.maxpool_backward(self.a, self.pool_out, pool_error_grad)\n        \n        # Backward pass through convolutional layer\n        conv_error_grad_relu = self.relu_backward(conv_error_grad, self.a_1)\n        dX, dW, db = conv2d_backward(conv_error_grad_relu, X, self.conv_kernel, self.conv_stride, self.conv_pad)\n        \n        # Update convolutional kernel\n        self.conv_kernel -= learning_rate * dW\n    \n    def maxpool_forward(self, input):\n        batch_size, in_channel, in_height, in_width = input.shape\n        output_height = (in_height - self.pool_size) // self.pool_stride + 1\n        output_width = (in_width - self.pool_size) // self.pool_stride + 1\n        \n        output = cp.array([\n            input[:, :, i * self.pool_stride:i * self.pool_stride + self.pool_size, j * self.pool_stride:j * self.pool_stride + self.pool_size]\n            for i in range(output_height) for j in range(output_width)\n        ]).reshape((batch_size, in_channel, output_height, output_width, self.pool_size, self.pool_size)).max(axis=(4, 5))\n        \n        return output\n    \n    def maxpool_backward(self, input, output, output_error_grad):\n        batch_size, in_channel, in_height, in_width = input.shape\n        output_height, output_width = output.shape[2], output.shape[3]\n        input_error_grad = cp.zeros_like(input)\n        \n        for i in range(output_height):\n            for j in range(output_width):\n                region = input[:, :, i * self.pool_stride:(i * self.pool_stride) + self.pool_size, j * self.pool_stride:(j * self.pool_stride) + self.pool_size]\n                max_region = cp.max(region, axis=(2, 3), keepdims=True)\n                mask = (region == max_region)\n                input_error_grad[:, :, i * self.pool_stride:(i * self.pool_stride) + self.pool_size,\n                                 j * self.pool_stride:(j * self.pool_stride) + self.pool_size] += mask * output_error_grad[:, :, i, j][:, :, None, None]\n        \n        return input_error_grad\n    \n    def train(self, X, y, epochs=10, learning_rate=1e-3, batch_size=21000):\n        for epoch in range(epochs):\n            for batch in range(42000//batch_size):\n                start = batch_size*batch\n                end = start+batch_size\n                X_batch = X[start:end]\n                label_batch = y[start:end]\n                output = self.forward(X_batch)\n                self.backward(X_batch, label_batch, learning_rate)\n                loss = self.mlp.cross_entropy_loss(output, label_batch)\n            print(f'Epoch {epoch+1}/{epochs}, Loss: {loss}')\n\n    def test(self, X):\n        r = cp.argmax(self.forward(X), axis=1)\n        r = cp.asnumpy(r)\n        submission = pd.DataFrame({\n        'ImageId': range(1, len(r) + 1),\n        'Label': r\n        })\n\n# Save the DataFrame to a CSV file\n        submission.to_csv(\"/kaggle/working/submission.csv\", index=False)\n\ncnn = CNN(X_train.shape, 10)\n\none_hot_encoded_y = one_hot_encoding(label_1)\ncnn.train(X_train, one_hot_encoded_y, epochs=100, learning_rate=0.3)\ncnn.test(test_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T14:29:11.829119Z","iopub.execute_input":"2025-01-04T14:29:11.829385Z","execution_failed":"2025-01-04T14:54:43.251Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/100, Loss: 2.5409883113930563\nEpoch 2/100, Loss: 2.3535367669833165\nEpoch 3/100, Loss: 2.3159065606456406\nEpoch 4/100, Loss: 2.3128064625246907\nEpoch 5/100, Loss: 2.310843676816196\nEpoch 6/100, Loss: 2.309351068161322\nEpoch 7/100, Loss: 2.3082192018056205\nEpoch 8/100, Loss: 2.3072247527143928\nEpoch 9/100, Loss: 2.306350403062044\nEpoch 10/100, Loss: 2.3056198899065117\nEpoch 11/100, Loss: 2.3049593067248435\nEpoch 12/100, Loss: 2.3043398131184483\nEpoch 13/100, Loss: 2.3037810599619006\nEpoch 14/100, Loss: 2.303253460609865\nEpoch 15/100, Loss: 2.302753656603038\nEpoch 16/100, Loss: 2.3022701428516945\nEpoch 17/100, Loss: 2.3018061174567412\nEpoch 18/100, Loss: 2.3013430110141595\nEpoch 19/100, Loss: 2.3009416356002887\nEpoch 20/100, Loss: 2.3004974652329455\nEpoch 21/100, Loss: 2.3000772749253326\nEpoch 22/100, Loss: 2.2996842787218075\nEpoch 23/100, Loss: 2.2993072899833003\nEpoch 24/100, Loss: 2.2989276828935377\nEpoch 25/100, Loss: 2.298543856429749\nEpoch 26/100, Loss: 2.29813073169519\nEpoch 27/100, Loss: 2.297734529319584\nEpoch 28/100, Loss: 2.297360733719555\nEpoch 29/100, Loss: 2.296968901706311\nEpoch 30/100, Loss: 2.2965917121279205\nEpoch 31/100, Loss: 2.2962403642061218\nEpoch 32/100, Loss: 2.295865310342372\nEpoch 33/100, Loss: 2.295510142343545\nEpoch 34/100, Loss: 2.2951703860179746\nEpoch 35/100, Loss: 2.2948178692717334\nEpoch 36/100, Loss: 2.294517855887896\nEpoch 37/100, Loss: 2.294097113758411\nEpoch 38/100, Loss: 2.2937670321635553\nEpoch 39/100, Loss: 2.293381657480243\nEpoch 40/100, Loss: 2.29302579761227\nEpoch 41/100, Loss: 2.2927064493072637\nEpoch 42/100, Loss: 2.292376359164916\nEpoch 43/100, Loss: 2.2919857408498645\nEpoch 44/100, Loss: 2.2916256748772708\nEpoch 45/100, Loss: 2.291275491628791\nEpoch 46/100, Loss: 2.2909162432034273\nEpoch 47/100, Loss: 2.2905375512146415\nEpoch 48/100, Loss: 2.2901809169559892\nEpoch 49/100, Loss: 2.2898426568066106\nEpoch 50/100, Loss: 2.2895009608111363\nEpoch 51/100, Loss: 2.289223549455325\nEpoch 52/100, Loss: 2.2890205331638054\nEpoch 53/100, Loss: 2.288663830391345\nEpoch 54/100, Loss: 2.2883872359438264\nEpoch 55/100, Loss: 2.287959492878906\nEpoch 56/100, Loss: 2.287612144035372\nEpoch 57/100, Loss: 2.287266112700564\nEpoch 58/100, Loss: 2.286815545998514\nEpoch 59/100, Loss: 2.2864275342194667\nEpoch 60/100, Loss: 2.286033065330405\nEpoch 61/100, Loss: 2.2857368600364363\nEpoch 62/100, Loss: 2.285607149850198\nEpoch 63/100, Loss: 2.285290166662895\nEpoch 64/100, Loss: 2.2849766436178256\nEpoch 65/100, Loss: 2.2845763460648625\nEpoch 66/100, Loss: 2.284193077784824\nEpoch 67/100, Loss: 2.2838902286057143\nEpoch 68/100, Loss: 2.2834843076975675\nEpoch 69/100, Loss: 2.2831144022089127\nEpoch 70/100, Loss: 2.2827859888339215\nEpoch 71/100, Loss: 2.282459868602291\nEpoch 72/100, Loss: 2.2821930608188277\nEpoch 73/100, Loss: 2.28181325007135\nEpoch 74/100, Loss: 2.2814089569798357\nEpoch 75/100, Loss: 2.2810466030363514\nEpoch 76/100, Loss: 2.2806566989720927\nEpoch 77/100, Loss: 2.2803725095521865\nEpoch 78/100, Loss: 2.2801607395756487\nEpoch 79/100, Loss: 2.2796840118996973\nEpoch 80/100, Loss: 2.279330970416257\nEpoch 81/100, Loss: 2.278903478212085\nEpoch 82/100, Loss: 2.2785095337363237\nEpoch 83/100, Loss: 2.2781134426584937\nEpoch 84/100, Loss: 2.2777640501183423\nEpoch 85/100, Loss: 2.2773187078191093\nEpoch 86/100, Loss: 2.277028830665195\nEpoch 87/100, Loss: 2.2768726756248165\nEpoch 88/100, Loss: 2.276722811169254\nEpoch 89/100, Loss: 2.276332200193812\nEpoch 90/100, Loss: 2.275909976644494\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"References :\n1. Hubel, D. H., & Wiesel, T. N. (1959a). Receptive fields of single neurons in the cats striate cortex. Retrieved from https://pmc.ncbi.nlm.nih.gov/articles/PMC1363130/pdf/jphysiol01298-0128.pdf\n2. He, K., Zhang, X., Ren, S., & Sun, J. (2015a). Delving deep into rectifiers: Surpassing human-level performance on ImageNet Classification. Retrieved from https://arxiv.org/abs/1502.01852\n3. Chellapilla, K., Puri, S., & Simard, P. (2006). High performance convolutional neural networks for document processing. Proceedings of the International Workshop on Frontiers in Handwriting Recognition (IWFHR-10).\n4. Kine, J. (2016, September 5). Backpropagation in Convolutional Neural Networks. Retrieved from https://www.jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/\n5. Aggarwal, C. C. (2018). Neural networks and deep learning: A textbook. Springer.\n6. Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems (pp. 1097-1105).\n7. Kingma, D. P., & Ba, J. (2014). Adam: A Method for Stochastic Optimization. Retrieved from https://arxiv.org/abs/1412.6980\n8. Tieleman, T., & Hinton, G. (2012). Lecture 6.5Rmsprop: Divide the gradient by a running average of its recent magnitude.\n9. Polyak, B. T. (1964). Some methods of speeding up the convergence of iteration methods. Journal of Applied Mathematics and Mechanics, 28(5), 984-993.\n10. S. Ioffe and C. Szegedy, Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift, arXiv.org, 2015. https://arxiv.org/abs/1502.03167\r\n","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}