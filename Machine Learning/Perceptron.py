"""
This is an implementation to perceptron(single layer), with arbitrary parameters
"""
from tkinter.tix import Tree
import cupy as cp
import random 
import matplotlib.pyplot as plt 


def perceptron(
        X, Y, W, b, learning_rate, epochs, training=True, plotting=True
):
    """
    Input : 
    X : Cupy array, Input vector, shape = (batch_size, features).
    Y : Cupy array, Label, either +1 or -1, shape = (batch_size, 1)
    W : Cupy array, Weights, if it is none then it will be automatically generated by standard normal distribution.
    b : 1D Cupy array, bias of perceptron.
    epoch : Training epochs.
    training : True or False, if False then it will not start training.

    Output:
    Loss of the perceptron and predicted class.
    """
    X = cp.array(X)
    Y = cp.array(Y)
    input_size = cp.shape(X)
    batch_size, feature_size = input_size[0], input_size[1]

    assert cp.shape(Y) == (batch_size,), f"The shape of Y is not correct, current shape : {Y.shape}, correct shape should be {(batch_size, )}"
    for y in Y:
        assert y==1 or y==-1, f"Label error, one or more elements are not 1 or -1"

    if W == None:
        W = cp.random.randn(
            feature_size, 1
        )
    if b==None:
        b = cp.random.randn(1)
    #Generate weights from standard normal distribution if W is not given 

    assert cp.shape(W) == (feature_size, 1), f"Weight shape is not correct, current shape : {W.shape}, correct shape should be {(feature_size, 1)}"
    def forward(X, W, b):
        M = []
        output =  cp.sign(cp.matmul(X, W) + b)
        for i in range(batch_size):
            if output[i] != Y[i]:
                M.append((X[i], Y[i]))
        return M
    
    def loss(X,Y,W,b):
        return -cp.sum(cp.array([Y[i]*(X[i]*W+b) for i in range(batch_size)]))

    if training == False:
        return loss(X,Y,W,b)
    
    elif training == True:
        for epoch in range(epochs):
            M = forward(X, W, b)
            if len(M)==0:
                print(f"Weight : {W}, bias : {b}")
                if plotting:
                    # Convert to numpy arrays for plotting
                    X_np = X.get()
                    Y_np = Y.get()
                    W_np = W.get().flatten()  # Shape (feature_size,)
                    b_np = b.get().item()     # Scalar value

                    # Create figure
                    plt.figure(figsize=(8, 6))
                    
                    # Plot data points
                    class1 = X_np[Y_np == 1]
                    class_neg1 = X_np[Y_np == -1]
                    plt.scatter(class1[:, 0], class1[:, 1], c='b', marker='o', label='Class +1')
                    plt.scatter(class_neg1[:, 0], class_neg1[:, 1], c='r', marker='x', label='Class -1')

                    # Create decision boundary
                    x_min = X_np[:, 0].min() - 1
                    x_max = X_np[:, 0].max() + 1

                    if W_np[1] != 0:  # Normal case
                        x_values = cp.array([x_min, x_max])
                        y_values = (-W_np[0] * x_values - b_np) / W_np[1]
                    else:  # Vertical line case
                        x_values = cp.full(2, -b_np / W_np[0])
                        y_values = cp.array([X_np[:, 1].min()-1, X_np[:, 1].max()+1])

                    # Convert to numpy for plotting
                    x_plot = x_values.get()
                    y_plot = y_values.get()

                    plt.plot(x_plot, y_plot, 'g--', label='Decision Boundary')
                    
                    # Add labels and legend
                    plt.xlabel('Feature 1')
                    plt.ylabel('Feature 2')
                    plt.title('Perceptron Decision Boundary')
                    plt.legend()
                    plt.grid(True)
                    plt.show()
                    
                break
            elif len(M)!=0:
                current_loss = loss(X,Y,W,b)
                print(f"Epoch {epoch}, loss = {current_loss}")
                picked_instance = random.choice(M)
                picked_instance : tuple 
                dL_dw = cp.multiply(picked_instance[0], picked_instance[1]).reshape((feature_size, 1))
                dL_db = picked_instance[1]
                assert cp.shape(dL_dw) == (feature_size, 1), f"Backprop error, current shape : {cp.shape(dL_dw)}, expected shape : {(feature_size, 1)}"
                W += learning_rate * dL_dw
                b += learning_rate * dL_db

X = cp.array(
    [
        [3,3], [4,3], [1,1], [0,0]
    ]
)
Y = cp.array(
    [1,1,-1,-1]
)
#Example Usage
perceptron(X, Y, W=None, b=None, learning_rate=0.5, epochs=50, training=True, plotting=True)